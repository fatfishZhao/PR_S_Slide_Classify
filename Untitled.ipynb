{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timeSeriesData', '__version__', '__header__', '__globals__']\n",
      "207.0\n",
      "[ 207.          213.          222.          233.66666667  250.33333333\n",
      "  275.15384615  293.5         311.84615385  336.16666667  347.83333333\n",
      "  367.38461538  382.          402.          415.75        425.11111111\n",
      "  432.          438.33333333  441.66666667  443.4       ]\n",
      "<type 'numpy.ndarray'>\n",
      "[[ 207.          213.          222.          233.66666667  250.33333333\n",
      "   275.15384615  293.5         311.84615385  336.16666667  347.83333333\n",
      "   367.38461538  382.          402.          415.75        425.11111111\n",
      "   432.          438.33333333  441.66666667  443.4       ]]\n",
      "<type 'numpy.ndarray'>\n",
      "[ array([[ 207.        ,  213.        ,  222.        ,  233.66666667,\n",
      "         250.33333333,  275.15384615,  293.5       ,  311.84615385,\n",
      "         336.16666667,  347.83333333,  367.38461538,  382.        ,\n",
      "         402.        ,  415.75      ,  425.11111111,  432.        ,\n",
      "         438.33333333,  441.66666667,  443.4       ]])]\n",
      "19\n",
      "1168\n",
      "最长的序列长度是264\n",
      "平均长度是66\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "from tensorflow.contrib.keras.api.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.api.keras.datasets import mnist\n",
    "from tensorflow.contrib.keras.api.keras.layers import LSTM,SimpleRNN,Dense,Activation\n",
    "from tensorflow.contrib.keras.api.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from tensorflow.contrib.keras.api.keras.utils import to_categorical\n",
    "# import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io as sio\n",
    "\n",
    "humanMat = sio.loadmat('humanData4py.mat')\n",
    "print humanMat.keys()\n",
    "print humanMat['timeSeriesData'][0][0][0][0] #最后一个0代表该序列数据的第一个数据\n",
    "print humanMat['timeSeriesData'][0][0][0] # 第三个0代表第一行吧（可能）\n",
    "print type(humanMat['timeSeriesData'][0][0][0])\n",
    "print humanMat['timeSeriesData'][0][0]#  第二个0代表这个cell的第一个元素\n",
    "print type(humanMat['timeSeriesData'][0][0])\n",
    "print humanMat['timeSeriesData'][0] # 第一个0代表第一个cell\n",
    "print humanMat['timeSeriesData'][0][0][0].size\n",
    "print humanMat['timeSeriesData'].size\n",
    "max = 0\n",
    "sum = 0\n",
    "for oneCell in humanMat['timeSeriesData']:\n",
    "    thisLength = oneCell[0][0].size\n",
    "    sum+=thisLength\n",
    "    if thisLength>max:\n",
    "        max = thisLength\n",
    "print '最长的序列长度是'+str(max)\n",
    "print '平均长度是'+str(sum/humanMat['timeSeriesData'].size)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', '__header__', '__globals__', 'keywords', 'timeSeriesData', '__version__']\n"
     ]
    }
   ],
   "source": [
    "robotMat = sio.loadmat('robotHCTSA.mat')\n",
    "print robotMat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预处理，将人类和机器数据都整理为2为数组，并且将所有时序数据补0至max数值\n",
    "# 还没做归一化\n",
    "humanSize = humanMat['timeSeriesData'].size # 人类样本的个数\n",
    "humanSeries = np.zeros([humanSize,max,1]) # 人类滑动时间序列\n",
    "for i in range(humanSize):\n",
    "    thisSeries = humanMat['timeSeriesData'][i][0]\n",
    "    tmpSeries = np.append(thisSeries,np.zeros([max-thisSeries[0].size,]))\n",
    "    humanMat['timeSeriesData'][i][0] = tmpSeries.reshape([1,max])\n",
    "    humanSeries[i] = humanMat['timeSeriesData'][i][0].reshape([-1,1])\n",
    "humanLabel = np.ones([humanSize,])\n",
    "\n",
    "robotSize = robotMat['timeSeriesData'].size # 机器样本个数\n",
    "robotSeries = np.zeros([robotSize,max,1]) # 机器滑动时间序列\n",
    "for i in range(robotSize):\n",
    "    thisSeries = robotMat['timeSeriesData'][i][0]\n",
    "    tmpSeries = np.append(thisSeries,np.zeros([max-thisSeries[0].size,]))\n",
    "    robotMat['timeSeriesData'][i][0] = tmpSeries.reshape([1,max])\n",
    "    robotSeries[i] = robotMat['timeSeriesData'][i][0].reshape([-1,1])\n",
    "robotLabel = np.zeros([robotSize,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 264, 1)\n",
      "(9720, 264, 1)\n",
      "(10888, 264, 1)\n",
      "(10888,)\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#随机分配训练与测试集\n",
    "print humanSeries.shape\n",
    "print robotSeries.shape\n",
    "X = np.append(humanSeries,robotSeries,axis=0)\n",
    "print X.shape\n",
    "y = np.append(humanLabel,robotLabel)\n",
    "print y.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "print y_train[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#建立神经网络\n",
    "TIME_STEPS = max\n",
    "INPUT_SIZE = 1\n",
    "CELL_SIZE = 50\n",
    "OUTPUT_SIZE = 2\n",
    "LR = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(\n",
    "    # for batch_input_shape, if using tensorflow as the backend, we have to put None for the batch_size.\n",
    "    # Otherwise, model.evaluate() will get error.\n",
    "    batch_input_shape=(None, TIME_STEPS, INPUT_SIZE),\n",
    "    units=32))\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('softmax'))\n",
    "adam = Adam(LR)\n",
    "model.compile(optimizer = adam,\n",
    "             loss = 'binary_crossentropy', # 使用二分类的损失函数\n",
    "              metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test cost: ', 0.68785446882247925, 'test accuracy: ', 0.51585972309112549)\n",
      "('test cost: ', 0.34742647409439087, 'test accuracy: ', 0.88981622457504272)\n",
      "('test cost: ', 0.34767472743988037, 'test accuracy: ', 0.88981622457504272)\n",
      "('test cost: ', 0.34697103500366211, 'test accuracy: ', 0.8898162841796875)\n",
      "('test cost: ', 0.34751343727111816, 'test accuracy: ', 0.88981622457504272)\n"
     ]
    }
   ],
   "source": [
    "# 训练与输出代价和准确率\n",
    "# training\n",
    "BATCH_SIZE = 50\n",
    "BATCH_INDEX = 0\n",
    "for step in range(2001):\n",
    "    # data shape = (batch_num, steps, inputs/outputs)\n",
    "    X_batch = X_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE, :, :]\n",
    "    Y_batch = y_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE,:]\n",
    "    cost = model.train_on_batch(X_batch, Y_batch)\n",
    "    BATCH_INDEX += BATCH_SIZE\n",
    "    BATCH_INDEX = 0 if BATCH_INDEX >= X_train.shape[0] else BATCH_INDEX\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        cost, accuracy = model.evaluate(X_test, y_test, batch_size=y_test.shape[0], verbose=False)\n",
    "        print('test cost: ', cost, 'test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[1 0 0 ..., 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 自带数据集测试\n",
    "from tensorflow.contrib.keras.api.keras.datasets import imdb\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print X_train.shape\n",
    "print y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[5 0 4 ..., 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print y_train.shape\n",
    "print y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90040213  0.09959786]\n",
      " [ 0.90039873  0.09960129]\n",
      " [ 0.90001541  0.09998464]\n",
      " [ 0.90040094  0.09959902]\n",
      " [ 0.9004029   0.0995971 ]\n",
      " [ 0.90040135  0.09959863]\n",
      " [ 0.90040094  0.09959906]\n",
      " [ 0.90065676  0.09934326]\n",
      " [ 0.90040076  0.09959926]\n",
      " [ 0.90040106  0.099599  ]\n",
      " [ 0.90069908  0.0993009 ]\n",
      " [ 0.9004001   0.09959991]\n",
      " [ 0.90040106  0.09959897]\n",
      " [ 0.9003998   0.09960025]\n",
      " [ 0.90037459  0.09962539]\n",
      " [ 0.90001541  0.09998462]\n",
      " [ 0.90029627  0.09970366]\n",
      " [ 0.90040046  0.09959958]\n",
      " [ 0.90040106  0.09959897]]\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict(X_train, batch_size=y_train.shape[0])\n",
    "print classes[1:20]\n",
    "print y_train[1:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
